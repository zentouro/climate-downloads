{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading ERA5 Data\n",
    "\n",
    "This code is forked/copied/stolen from Kevin: https://github.com/ks905383/climate-downloads/blob/main/download_ERA5.ipynb\n",
    "\n",
    "ERA5 data is downloaded using the NetCDF download protocols. Some variables may only be available in GRIB format - however, all standard climatological variables will be available using this script. ERA5 files are available in hourly timesteps; this code allows for resampling the data at lower frequencies (see the section on [Subsetting and Resampling](https://github.com/ks905383/climate-downloads/blob/40578fc927bac655fcfa3d59738d4cc81077423f//#subset)).\n",
    "\n",
    "The ERA5 API must be installed first; follow this [guide](https://cds.climate.copernicus.eu/api-how-to) for more information. (the easiest way is to use `pip` to install `cdsapi`).\n",
    "\n",
    "ERA5 files are 0.25ยบ hourly data - this results in very large files. This code is built on the assumption that for stability of the download and efficiency of memory use, data is downloaded one temporal chunk at a time (by default one year at a time). For reference, a 20 x 60 degree box by the equator of one variable takes around 10 minutes per year to prepare and download. Each temporal chunk is downloaded separately and saved in a temporary file (these files are useful for checking if the download is working correctly, and to start working with data) - once all the temporary files are downloaded, they are concatenated into one single file for the whole temporal range, and the temporary files are deleted. Set a bounding box for the download in `geographic_vars`.\n",
    "\n",
    "Currently, only multiples of full years at a time can be downloaded; changing this is a relatively easy fix in the `c.retrieve()` call if it is needed.\n",
    "\n",
    "The code checks for previously downloaded and processed files (based on the output temporal resolution and subset) and ignores them with a message (this is the case for both temporary and concatenated files - but previously downloaded temporary files are of course included in the concatenated file either way).\n",
    "\n",
    "Files are placed in a subdirectory of the `raw_data_dir` set in the `dwnld_config.py` file `[raw_data_dir]/ERA5/`. If this subdirectory doesn't yet exist, it is created. Output files have the following filename:\n",
    "\n",
    "`[raw_data_dir]/ERA5/[short_name]_[output_freq_name]_ERA5_historical_reanalysis_[allyears[0]0101-allyears[-1]1231]_[fn_suffix].nc`\n",
    "\n",
    "with `[raw_data_dir]` set from the `config` file, `[short_name]` set in the `download_vars`, `output_freq_name` set in the `resampling_vars` (unless no time resampling is conducted; in this case `_hr_` is used), and `fn_suffix` set in the `geographic_vars`.\n",
    "\n",
    "Summary of changes to downloaded ERA5 file\n",
    "- filename changed to CMIP5 standard, from the procedurally-generated unintelligible ERA5 file\n",
    "- \"latitude\" changed to \"lat\" and \"longitude\" changed to \"lon\"\n",
    "- variable short named changed to `download_vars['short_name']`\n",
    "- data is resampled temporally (optional) from hourly to anything allowed by the `pandas` resampling syntax\n",
    "\n",
    "Useful links Copernicus Data Store (where ERA5 data is located) data download page: https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=form\n",
    "\n",
    "ERA5 on the UCAR Climate Data Guide: https://climatedataguide.ucar.edu/climate-data/era5-atmospheric-reanalysis\n",
    "\n",
    "ERA5 documentation: https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config file \n",
    "import dwnld_config as cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "Set which variables to download. \n",
    "- `short_name`: variable shorthand (e.g. `tas`) used as filename identifier (e.g. `tas_*.nc`) in the output filename and as the variable name in the file itself\n",
    "- `long_name`: ERA5 variable identifier used to find file to download\n",
    "\n",
    "The `long_name` for each variable can be found in the [ERA5 Parameter Database](https://apps.ecmwf.int/codes/grib/param-db) - for files available in NetCDF (filter using the NetCDF button on the left), the name will be listed as \"cfName\" in the table at the bottom of the page for each relevant variable, under the NetCDF tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which variables to download\n",
    "download_vars = [\n",
    "    {'short_name': 'tp', \n",
    "    'long_name': 'total_precipitation'}, \n",
    "    {'short_name': 'swvl1', \n",
    "    'long_name': 'volumetric_soil_water_layer_1'}, \n",
    "    {'short_name': 'swvl2', \n",
    "    'long_name': 'volumetric_soil_water_layer_2'},\n",
    "    {'short_name': 'swvl3', \n",
    "    'long_name': 'volumetric_soil_water_layer_3'},\n",
    "    {'short_name': '2mt', \n",
    "     'long_name': '2m_temperature'},\n",
    "    {'short_name': 'sf', \n",
    "     'long_name': 'snowfall'},     \n",
    "    {'short_name': 'sd', \n",
    "     'long_name': 'snow_depth'}, \n",
    "    {'short_name': 'evap', \n",
    "     'long_name': 'evaporation'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"subset\"></a>Subsetting and Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### regions\n",
    "## WNA (western North America): [50, -130, 25, -100] (execept i accidentally processed it to 100... not -100)\n",
    "## north: 50, west: -130, east: -100, south: 25\n",
    "\n",
    "\n",
    "# Set geographic extent of download\n",
    "geographic_vars = {'bbox':[50, -130, 25, 100], # Max lat, min lon, min lat, max lon\n",
    "                   #'fn_suffix':'_WNA'} ## regional sufix for file saving\n",
    "                   'fn_suffix': '_50.-130.25.100'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set years to process\n",
    "all_years = np.arange(1979,2024)\n",
    "\n",
    "# How many years to be processed at a time (may impact performance; detailed checks forthcoming)\n",
    "chunk_size = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERA5 data is released as hourly data. This code supports resampling using the `xarray.Dataset.resample()` function ([docs here](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.resample.html)), which is based on the `pandas.resample()` functionality. The next cell adjusts whether data is resampled. \n",
    "\n",
    "The `resample_level` value is piped into the `freq` call of the `.resample` indexer. Useful, common options include: \n",
    "- `D`: daily\n",
    "- `M`: month (end)\n",
    "- `MS`: month (beginning)\n",
    "\n",
    "A full list of options is available in the `pandas` documentation [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set resampling\n",
    "resampling_vars = {'resamp':True, # whether to resample from hourly (if false, the rest is ignored)\n",
    "                   'resample_level':'D', # what level to resample to \n",
    "                   'output_freq_name':'day'} # what the frequency should be called in the filename, attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.lpaths['raw_data_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(cfg.lpaths['raw_data_dir']):\n",
    "    os.mkdir(cfg.lpaths['raw_data_dir'])\n",
    "    print(cfg.lpaths['raw_data_dir']+' created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break up years into chunks\n",
    "chunks = [all_years[i:i + chunk_size] for i in range(0, len(all_years), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cdsapi.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_day_ERA5_historical_reanalysis_19890101-19931231_WNA_CNA_NCA_America.nc already exists; skipped.\n",
      "/Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_day_ERA5_historical_reanalysis_19890101-19931231_WNA_CNA_NCA_America.nc already exists; skipped.\n",
      "/Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_day_ERA5_historical_reanalysis_19890101-19931231_WNA_CNA_NCA_America.nc already exists; skipped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 11:58:19,699 INFO Welcome to the CDS\n",
      "2024-01-12 11:58:19,701 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n",
      "2024-01-12 11:58:19,972 INFO Downloading https://download-0018.copernicus-climate.eu/cache-compute-0018/cache/data8/adaptor.mars.internal-1705075917.3186674-12616-18-de001017-9623-4632-beb0-b340815a2859.nc to /Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_hr_ERA5_historical_reanalysis_19940101-19981231_WNA_CNA_NCA_America.nc (1.9G)\n",
      "2024-01-12 12:02:24,052 INFO Download rate 7.8M/s                             \n",
      "2024-01-12 12:02:48,064 INFO Welcome to the CDS\n",
      "2024-01-12 12:02:48,064 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_day_ERA5_historical_reanalysis_19940101-19981231_WNA_CNA_NCA_America.nc processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 12:02:48,218 INFO Request is queued\n",
      "2024-01-12 12:06:40,817 WARNING Connection error: [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Read timed out. (read timeout=60)]. Attempt 1 of 500.\n",
      "2024-01-12 12:06:40,820 WARNING Retrying in 120 seconds\n",
      "2024-01-12 12:08:40,824 INFO Retrying now...\n",
      "2024-01-12 12:16:10,022 INFO Request is running\n",
      "2024-01-12 13:10:59,030 INFO Request is completed\n",
      "2024-01-12 13:10:59,033 INFO Downloading https://download-0021.copernicus-climate.eu/cache-compute-0021/cache/data9/adaptor.mars.internal-1705081980.1275408-7068-5-5772b454-f4db-4852-b7ca-aae5e088bafd.nc to /Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_hr_ERA5_historical_reanalysis_19990101-20031231_WNA_CNA_NCA_America.nc (1.9G)\n",
      "2024-01-12 13:25:11,499 INFO Download rate 2.2M/s                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_day_ERA5_historical_reanalysis_19990101-20031231_WNA_CNA_NCA_America.nc processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 13:25:37,472 INFO Welcome to the CDS\n",
      "2024-01-12 13:25:37,481 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n",
      "2024-01-12 13:25:37,746 INFO Request is queued\n",
      "2024-01-12 13:31:58,915 INFO Request is running\n",
      "2024-01-12 14:26:13,138 INFO Request is completed\n",
      "2024-01-12 14:26:13,140 INFO Downloading https://download-0020.copernicus-climate.eu/cache-compute-0020/cache/data8/adaptor.mars.internal-1705086514.0610776-19103-6-9ade761c-1740-434d-8065-fe1866a8ce3e.nc to /Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_hr_ERA5_historical_reanalysis_20040101-20081231_WNA_CNA_NCA_America.nc (1.9G)\n",
      "2024-01-12 14:31:41,570 INFO Download rate 5.8M/s                             \n",
      "2024-01-12 14:32:05,598 INFO Welcome to the CDS\n",
      "2024-01-12 14:32:05,599 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_day_ERA5_historical_reanalysis_20040101-20081231_WNA_CNA_NCA_America.nc processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 14:32:05,831 INFO Request is queued\n",
      "2024-01-12 14:38:26,582 INFO Request is running\n",
      "2024-01-12 15:37:54,155 INFO Request is completed\n",
      "2024-01-12 15:37:54,158 INFO Downloading https://download-0010-clone.copernicus-climate.eu/cache-compute-0010/cache/data4/adaptor.mars.internal-1705090708.9765384-21143-17-d3e4a676-10b4-48f2-b7a7-ae1ee5050d44.nc to /Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_hr_ERA5_historical_reanalysis_20090101-20131231_WNA_CNA_NCA_America.nc (1.9G)\n",
      "2024-01-12 15:47:01,522 INFO Download rate 3.5M/s                             \n",
      "2024-01-12 15:47:25,707 INFO Welcome to the CDS\n",
      "2024-01-12 15:47:25,708 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_day_ERA5_historical_reanalysis_20090101-20131231_WNA_CNA_NCA_America.nc processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 15:47:25,927 INFO Request is queued\n",
      "2024-01-12 15:49:20,968 INFO Request is running\n",
      "2024-01-12 15:59:47,764 INFO Request is completed\n",
      "2024-01-12 15:59:47,765 INFO Downloading https://download-0009-clone.copernicus-climate.eu/cache-compute-0009/cache/data0/adaptor.mars.internal-1705093138.0195093-9787-1-caa46ad2-2c7d-457c-9f67-35fdfb46ba03.nc to /Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_hr_ERA5_historical_reanalysis_20140101-20141231_WNA_CNA_NCA_America.nc (379.3M)\n",
      "2024-01-12 16:00:35,684 INFO Download rate 7.9M/s                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miriam/Documents/02_academia/02_PhD/research/raw_data/ERA5/tas_day_ERA5_historical_reanalysis_20140101-20141231_WNA_CNA_NCA_America.nc processed!\n"
     ]
    }
   ],
   "source": [
    "for download_var in download_vars:\n",
    "    \n",
    "    if resampling_vars['resamp']:\n",
    "        fn_final = (cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                    '_'+resampling_vars['output_freq_name']+\n",
    "                    '_ERA5_historical_reanalysis_'+\n",
    "                    str(all_years[0])+'0101-'+str(all_years[-1])+'1231'+\n",
    "                    geographic_vars['fn_suffix']+'.nc')\n",
    "    else:\n",
    "        fn_final = (cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                    '_hr_ERA5_historical_reanalysis_'+\n",
    "                    str(all_years[0])+'0101-'+str(all_years[-1])+'1231'+\n",
    "                    geographic_vars['fn_suffix']+'.nc')\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(fn_final):\n",
    "\n",
    "        for chunk in tqdm(chunks):\n",
    "            # Set filename for temporary file\n",
    "            fn0 = (cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                   '_hr_ERA5_historical_reanalysis_'+str(chunk[0])+'0101-'+str(chunk[-1])+'1231'+geographic_vars['fn_suffix']+'.nc')\n",
    "            \n",
    "            # If the file does not yet exist, download it\n",
    "            if not os.path.exists(re.sub('hr','day',fn0)): \n",
    "\n",
    "                ## retrieval request\n",
    "                dataset = \"reanalysis-era5-single-levels\"\n",
    "                request = {\n",
    "                    \"product_type\": [\"reanalysis\"],\n",
    "                    'variable': download_var['long_name'],\n",
    "                    'year': [str(ch) for ch in chunk],\n",
    "                    \"month\": [\n",
    "                        \"01\", \"02\", \"03\",\n",
    "                        \"04\", \"05\", \"06\",\n",
    "                        \"07\", \"08\", \"09\",\n",
    "                        \"10\", \"11\", \"12\"\n",
    "                    ],\n",
    "                    \"day\": [\n",
    "                        \"01\", \"02\", \"03\",\n",
    "                        \"04\", \"05\", \"06\",\n",
    "                        \"07\", \"08\", \"09\",\n",
    "                        \"10\", \"11\", \"12\",\n",
    "                        \"13\", \"14\", \"15\",\n",
    "                        \"16\", \"17\", \"18\",\n",
    "                        \"19\", \"20\", \"21\",\n",
    "                        \"22\", \"23\", \"24\",\n",
    "                        \"25\", \"26\", \"27\",\n",
    "                        \"28\", \"29\", \"30\",\n",
    "                        \"31\"\n",
    "                    ],\n",
    "                    \"time\": [\n",
    "                        \"00:00\", \"01:00\", \"02:00\",\n",
    "                        \"03:00\", \"04:00\", \"05:00\",\n",
    "                        \"06:00\", \"07:00\", \"08:00\",\n",
    "                        \"09:00\", \"10:00\", \"11:00\",\n",
    "                        \"12:00\", \"13:00\", \"14:00\",\n",
    "                        \"15:00\", \"16:00\", \"17:00\",\n",
    "                        \"18:00\", \"19:00\", \"20:00\",\n",
    "                        \"21:00\", \"22:00\", \"23:00\"\n",
    "                    ],\n",
    "                    \"data_format\": \"netcdf\",\n",
    "                    \"download_format\": \"unarchived\",\n",
    "                    'area': geographic_vars['bbox']\n",
    "                }\n",
    "\n",
    "                c.retrieve(dataset, request, fn0)\n",
    "\n",
    "                tmp = xr.open_dataset(fn0)\n",
    "\n",
    "                # Rename variables\n",
    "                #tmp = tmp.rename({'latitude':'lat','longitude':'lon'})\n",
    "                ## not sure if all files will have valid_time\n",
    "                tmp = tmp.rename({'latitude':'lat',\n",
    "                                  'longitude':'lon', \n",
    "                                  'valid_time': 'time'})\n",
    "\n",
    "                ## probably should add something to clean up grib formatting \n",
    "\n",
    "                if download_var['short_name'] not in tmp.var().variables.keys():\n",
    "                    tmp = tmp.rename({[k for k in tmp.var().variables.keys()][0]:download_var['short_name']})\n",
    "\n",
    "                if resampling_vars['resamp']:\n",
    "                    # Resample temporally\n",
    "                    tmp = tmp.resample(time=resampling_vars['resample_level']).mean()\n",
    "                    fn1 = re.sub('hr',resampling_vars['output_freq_name'],fn0)\n",
    "                    \n",
    "                    # Export\n",
    "                    tmp.to_netcdf(fn1)\n",
    "\n",
    "                    # Remove old file\n",
    "                    os.remove(fn0)\n",
    "                else:\n",
    "                    fn1 = fn0\n",
    "\n",
    "                    # Remove old file (reverse order from the resampling case \n",
    "                    # above, which gives a new filename, because to_netcdf \n",
    "                    # doesn't overwrite)\n",
    "                    os.remove(fn0)\n",
    "                    \n",
    "                    # Export\n",
    "                    tmp.to_netcdf(fn1)\n",
    "\n",
    "                print(fn1+' processed!')\n",
    "                \n",
    "            else: \n",
    "                #print(fn1+' already exists; skipped.')\n",
    "                print(fn0+' already exists; skipped.')\n",
    "\n",
    "        # Open all the files (by wildcarding the date filename segment)\n",
    "        ds = xr.open_mfdataset(cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                                '_'+resampling_vars['output_freq_name']+\n",
    "                                '_ERA5_historical_reanalysis_*'+geographic_vars['fn_suffix']+'.nc',\n",
    "                               combine='by_coords')\n",
    "\n",
    "        # Save the concatenated file across all years \n",
    "        ds.to_netcdf(fn_final)\n",
    "\n",
    "        # Remove the component files \n",
    "        for chunk in chunks:\n",
    "            os.remove(cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                       '_'+resampling_vars['output_freq_name']+\n",
    "                       '_ERA5_historical_reanalysis_'+\n",
    "                       str(chunk[0])+'0101-'+str(chunk[-1])+'1231'+\n",
    "                       geographic_vars['fn_suffix']+'.nc')\n",
    "    else:\n",
    "        print(fn_final+' already exists, skipped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
